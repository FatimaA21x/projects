{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e8a5e5-1576-4b06-b057-583fc5c4930c",
   "metadata": {},
   "source": [
    "# Creating a Neural Network From Scratch (Only Using Numpy)\n",
    "## About the Project\n",
    "The purpose of this project is to create a simple neural network from scratch by using only the Python package Numpy to enhance my understanding of neural networks, the maths behind it all, and how they are implemented.\n",
    "This specific neural network aims to decipher 28 x 28 pixel images of handwritten numbers. \n",
    "The network will consist of an input layer, a hidden layer, and an output layer. \n",
    "With ten neurons in both the hidden layer and the output layer.\n",
    "I will try to classify the data from the famous digit MNIST dataset using this neural network.\n",
    "\n",
    "## About the Dataset\n",
    "The dataset is part of a Kaggle competition found [here](https://www.kaggle.com/c/digit-recognizer/data)\n",
    "\n",
    "The dataset consists of 42,000 28 x 28 pixel handwritten digit images.\n",
    "There are 785 features in the dataset:\n",
    "* **label**: *The true number of the image*\n",
    "* **pixel**: *The other 784 features correspond to one of the pixels of the image*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d536bb-77b7-48af-8841-78f97f26ccda",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d0996864-1ec2-458a-9c5f-116ef9454759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684d2f7-12fd-4ee8-91ab-c9735b874a06",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45b3614e-a528-4f4f-8edc-f9fd09eac974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"path/Number_Data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4443eabb-4318-4a7a-952e-e05792e38161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to work with arrays so that we can do the calculations required for the neural network\n",
    "data = np.array(data)\n",
    "\n",
    "m, n = data.shape\n",
    "# m = number of rows/number of images\n",
    "# n = number of columns/number of pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177308ef-66da-4370-9db0-fd6f4dfaace3",
   "metadata": {},
   "source": [
    "### Split Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "85817c3f-121c-44a2-aa17-034c80dae8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle images randomly\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Select the first 1,000 images for the test data\n",
    "data_test = data[0:1000].T \n",
    "# Transpose data so that each image is in one column instead of one row.\n",
    "# This is required for dot products\n",
    "Y_test = data_test[0] # This contains the labels for each image. the true values.\n",
    "X_test = data_test[1:n] # This contains the pixel values for every image.\n",
    "X_test = X_test / 255\n",
    "\n",
    "# Select the rest of the 41,000 images for the train data\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27d7b8b-e357-485a-9ba1-16b58bbb33f8",
   "metadata": {},
   "source": [
    "## Creating Functions for the Neural Networks\n",
    "**Functions:**\n",
    "1. one_hot(Y): *Encodes predicted outcomes*\n",
    "2. init_params(): *Outputs random parameters*\n",
    "3. ReLU(Z): *Activation function to ensure non-linearity*\n",
    "4. deriv_ReLU(Z): *Derivative of activation function used in back propagation*\n",
    "5. softmax(Z): *Activation function that ouputs probabilities*\n",
    "6. forward_prop(W1, b1, W2, b2, X): *Produce predicted output*\n",
    "7. back_prop(Z1, A1, Z2, A2, W2, X, Y): *Calculates loss from each parameter*\n",
    "8. update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha): *Changes parameters with respect to loss contribution*\n",
    "11. gradient_descent(X, Y, iterations, alpha): *Essentialy the neural network*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4bbae-a810-45e5-aa20-c609d16cc732",
   "metadata": {},
   "source": [
    "### one_hot(Y)\n",
    "This function is used to calculate the loss of the neural network in back propogation. If the specified label is 4 all predicted outcomes as 4 will be encoded as 1. \n",
    "\n",
    "Input: \n",
    "- Y: True label of each image\n",
    "\n",
    "Output:\n",
    "- one_hot_Y: Encoded values of predicted labels of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c40e51eb-0dc2-4b39-bd87-65276844ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1)) # Creates the correctly sized matrix\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1 # For each row specified in the label in Y and set it to 1.\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed5980-de93-48d1-a75b-829d2b79f7f9",
   "metadata": {},
   "source": [
    "### init_params()\n",
    "\n",
    "Assigns random weights and biases for all nodes in the neural network.\n",
    "\n",
    "Inputs: None\n",
    "\n",
    "Outputs: \n",
    "- W1: weights for each input for each node in the hidden layer \n",
    "- b1: biases for each node in the hidden layer\n",
    "- W2: weights for each input for each node in the output layer\n",
    "- b2: biases for each node in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea6bc04d-2031-400e-b259-38698adac45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10)\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94c734-16e2-4cd2-b12e-b82c74fd4c9d",
   "metadata": {},
   "source": [
    "### ReLU(Z) + deriv_ReLU(Z)\n",
    "The Rectified Linear Unit (ReLU) is an activation function where any values above 0 are unchanged and any below 0 are changed to 0.\n",
    "Without the use of this function the neural network will work as a combination of linear functions thus becoming a large linear regression model.\n",
    "\n",
    "Input:\n",
    "- Z: unactivated input values \n",
    "\n",
    "Output:\n",
    "- A: Activated values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9f640a39-a1d2-49b5-a342-b540ad5994f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0) \n",
    "    # Goes through each element.\n",
    "    # If Z > 0 = Z, Z < 0 = 0\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47b91a-090a-41ca-b71a-f07cad56015b",
   "metadata": {},
   "source": [
    "### softmax(Z)\n",
    "The softmax function is another activation function, however it is quite different to ReLU as it converts the outputs of the hidden layer into probabilites where 1 is absolute certainity and 0 is no chance. Therefore, the neural network can classify the output as one fo the 10 digits.\n",
    "\n",
    "Inputs:\n",
    "- Z: unactivated input values for output layer\n",
    "\n",
    "Outputs:\n",
    "- A: activated values for output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b38e0c22-9a8d-42ab-808a-c9af0fd5f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb59ab6-bc50-4863-9ab7-4bfc65a92118",
   "metadata": {},
   "source": [
    "### forward_prop(W1, b1, W2, b2, X)\n",
    "This forward propagation function will input the randomly initialised weights and biases and produce an ouptut of predicted outcomes.\n",
    "\n",
    "Inputs: \n",
    "- W1: weights for each input for each node in the hidden layer \n",
    "- b1: biases for each node in the hidden layer\n",
    "- W2: weights for each input for each node in the output layer\n",
    "- b2: biases for each node in the output layer\n",
    "- X: Pixel values of the images\n",
    "\n",
    "Outputs:\n",
    "- Z1: unactivated values of hidden layer\n",
    "- A1: activated input values of hidden layer\n",
    "- Z2: unactivated values of input from output layer\n",
    "- A2: predictions of labels for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd25947d-dc57-43f7-9aa1-789c240358c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28464d3-74c4-400a-9eb4-9041b5692092",
   "metadata": {},
   "source": [
    "### back_prop(Z1, A1, Z2, A2, W2, Y)\n",
    "This back propagation function will use the predicted outcomes from the forward propogation function to calculate the loss from the true labels. The function then precedes to calcualte how much each of the parameters contributed to this loss by differentiating each calculation. \n",
    "\n",
    "Inputs:\n",
    "- Z1: unactivated values of input layer\n",
    "- A1: activated input values\n",
    "- Z2: unactivated values of input from hidden layer\n",
    "- A2: predictions of labels for each image\n",
    "- W1: weights for each input for each node in the hidden layer \n",
    "- W2: weights for each input for each node in the output layer\n",
    "- X: Pixel values of the images\n",
    "- Y: True labels of each image\n",
    "\n",
    "Outputs:\n",
    "- dW1: The error contributed by the weights in the input layer\n",
    "- db1: The error contributed by the biases in the input layer\n",
    "- dw2: The error contributed by the weights in hidden layer\n",
    "- db2: the error contributed by the biases in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "970f8a52-f1fd-46d7-af0f-1808144ac3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    m = Y.size\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1/m * dZ2.dot(A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1)\n",
    "    dW1 = 1/m * dZ1.dot(X.T)\n",
    "    db1 = 1/m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353de2bf-2721-4ed3-8657-cbad4100b649",
   "metadata": {},
   "source": [
    "### update_params(W1, b1, W2, b2, dW1, db1, dW2. db2, alpha)\n",
    "This function updates the parameters with respect to their error contribution so that total loss is minimised in the predicted output for the next run of the neural network.\n",
    "\n",
    "Inputs:\n",
    "- W1: weights for each input for each node in the hidden layer \n",
    "- b1: biases for each node in the hidden layer\n",
    "- W2: weights for each input for each node in the output layer\n",
    "- b2: biases for each node in the output layer\n",
    "- dW1: The error contributed by the weights in the input layer\n",
    "- db1: The error contributed by the biases in the input layer\n",
    "- dw2: The error contributed by the weights in hidden layer\n",
    "- db2: the error contributed by the biases in the hidden layer\n",
    "- alpha: The learning rate of the neural network\n",
    "\n",
    "Outputs:\n",
    "- W1: weights for each input for each node in the hidden layer \n",
    "- b1: biases for each node in the hidden layer\n",
    "- W2: weights for each input for each node in the output layer\n",
    "- b2: biases for each node in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08707c05-7bcc-411c-820d-14d1bdf65701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0ad2a-ef3e-4a23-b19b-4e6afa7bf69e",
   "metadata": {},
   "source": [
    "### get_predictions(A2) + get_accuracy(predictions, Y)\n",
    "These functions display the performance of the neural network.\n",
    "The get_predictions function simply returns the predictions made by the neural network.\n",
    "The get_accuracy function simply calculates the proportion of the predictions that are correct (match with the true label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f68e7e45-47b3-4613-a11c-5c836fe4d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca18b02-c84d-44bd-a8eb-9c3c0ce0200d",
   "metadata": {},
   "source": [
    "### gradient_descent(X, Y, iterations, alpha)\n",
    "The gradient_descent function is essentialy the neural network. It combines previous functions to initalise random paramters and then continuously goes through a loop of creating an output, calculating loss, and updating parameters to minimise loss and provide (hopefully) accurate predictions.\n",
    "\n",
    "Inputs:\n",
    "- X: Pixel values of the images\n",
    "- Y: True labels of each image\n",
    "- iterations: The number of times the neural network loops through forward and backward propagation\n",
    "- alpha: The learning rate of the neural network\n",
    "\n",
    "Outputs:\n",
    "- W1: The optimal weights for each input for each node in the hidden layer \n",
    "- b1: The optimal biases for each node in the hidden layer\n",
    "- W2: The optimal weights for each input for each node in the output layer\n",
    "- b2: The optimal biases for each node in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "391c2c3a-e825-4b6d-8e77-8f0718f5d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(\"Accuracy: \", get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f6e0b-dbd3-4d64-89e4-4921b43065a1",
   "metadata": {},
   "source": [
    "## Running the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "22560662-ed3e-4190-9e9a-c8a2401d3aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[4 6 7 ... 0 1 6] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.10004878048780488\n",
      "Iteration:  10\n",
      "[7 7 7 ... 0 1 3] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.16229268292682927\n",
      "Iteration:  20\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.25514634146341464\n",
      "Iteration:  30\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.32821951219512197\n",
      "Iteration:  40\n",
      "[0 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.38970731707317074\n",
      "Iteration:  50\n",
      "[0 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.4453658536585366\n",
      "Iteration:  60\n",
      "[0 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.4937560975609756\n",
      "Iteration:  70\n",
      "[0 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.5364878048780488\n",
      "Iteration:  80\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.5702926829268292\n",
      "Iteration:  90\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.5980243902439024\n",
      "Iteration:  100\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.6203414634146341\n",
      "Iteration:  110\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.6384878048780488\n",
      "Iteration:  120\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.6558048780487805\n",
      "Iteration:  130\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.6701219512195122\n",
      "Iteration:  140\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.6834878048780488\n",
      "Iteration:  150\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.6961219512195121\n",
      "Iteration:  160\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7062195121951219\n",
      "Iteration:  170\n",
      "[4 1 7 ... 4 1 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7152195121951219\n",
      "Iteration:  180\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.723609756097561\n",
      "Iteration:  190\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7315365853658536\n",
      "Iteration:  200\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7383170731707317\n",
      "Iteration:  210\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7451463414634146\n",
      "Iteration:  220\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.751780487804878\n",
      "Iteration:  230\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7576341463414634\n",
      "Iteration:  240\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7636585365853659\n",
      "Iteration:  250\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7688780487804878\n",
      "Iteration:  260\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.773829268292683\n",
      "Iteration:  270\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7789756097560976\n",
      "Iteration:  280\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7835365853658537\n",
      "Iteration:  290\n",
      "[4 1 7 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7881463414634147\n",
      "Iteration:  300\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7928536585365854\n",
      "Iteration:  310\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.7968536585365854\n",
      "Iteration:  320\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8004146341463415\n",
      "Iteration:  330\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8037317073170732\n",
      "Iteration:  340\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8077073170731708\n",
      "Iteration:  350\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.811439024390244\n",
      "Iteration:  360\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8140243902439024\n",
      "Iteration:  370\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8171951219512195\n",
      "Iteration:  380\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.820390243902439\n",
      "Iteration:  390\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8233414634146341\n",
      "Iteration:  400\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8258048780487804\n",
      "Iteration:  410\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8285365853658536\n",
      "Iteration:  420\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8304634146341463\n",
      "Iteration:  430\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8327073170731707\n",
      "Iteration:  440\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8348048780487805\n",
      "Iteration:  450\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8365609756097561\n",
      "Iteration:  460\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8381951219512195\n",
      "Iteration:  470\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8399512195121951\n",
      "Iteration:  480\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8418048780487805\n",
      "Iteration:  490\n",
      "[4 1 9 ... 4 9 1] [4 1 7 ... 4 4 1]\n",
      "Accuracy:  0.8437073170731707\n"
     ]
    }
   ],
   "source": [
    "# Training the neural network with the train data to find the optimal parameters.\n",
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 500, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc51421-cf35-4089-ad37-51bc4c301d9b",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9d236962-7a96-437f-83cf-700e47b35078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that essentialy show one example of the neural network's predicted outcome compared to the true label\n",
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions\n",
    "\n",
    "def test_predictions(index, W1, b1, W2, b2):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation = \"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5d09387d-e5d9-4887-bd12-24dbeda966e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [5]\n",
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions(15, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d44ac23f-b951-472f-b905-9f484b181b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 7 0 5 0 7 4 2 9 0 6 8 1 1 6 8 9 6 1 2 1 1 9 9 5 2 5 7 9 2 5 1 5 8 1 3 0\n",
      " 1 8 6 2 7 1 0 8 9 9 2 6 5 4 1 2 6 5 6 9 4 8 5 2 8 0 7 2 6 0 6 3 1 2 6 8 2\n",
      " 3 2 1 8 5 2 7 9 9 2 5 0 6 9 5 8 8 2 8 3 3 5 0 6 2 6 6 7 4 8 4 7 2 2 5 1 4\n",
      " 2 3 7 2 2 6 8 1 8 8 4 4 1 8 9 3 9 2 1 6 0 2 4 3 9 3 6 1 0 9 6 5 3 3 6 0 0\n",
      " 0 5 2 9 4 1 8 7 4 1 2 6 6 6 9 3 6 6 0 8 1 0 6 5 6 9 6 3 9 4 2 2 0 3 8 3 3\n",
      " 8 5 6 6 6 7 0 1 2 8 5 0 9 5 6 8 1 9 2 1 6 4 0 8 9 7 4 6 2 5 1 0 4 3 8 6 9\n",
      " 5 7 4 4 9 9 4 3 3 1 7 9 5 1 5 9 4 3 3 7 4 5 6 9 0 7 7 4 8 4 4 8 8 2 2 8 4\n",
      " 6 2 9 2 1 4 8 4 6 2 1 0 9 8 6 1 8 0 4 1 2 1 4 1 1 4 1 8 5 8 7 4 0 8 1 5 4\n",
      " 2 0 7 9 1 1 9 2 0 1 2 1 8 7 1 0 3 9 4 6 8 7 4 9 7 5 1 0 2 3 6 3 3 7 0 2 3\n",
      " 2 1 6 4 9 5 4 0 9 1 0 8 6 0 1 7 7 3 3 3 7 9 4 0 3 9 2 2 2 6 2 7 2 4 5 7 1\n",
      " 5 1 6 2 8 1 3 6 0 6 8 1 6 5 2 8 6 4 6 0 9 5 7 8 6 9 2 3 1 1 4 3 6 0 3 6 6\n",
      " 7 5 8 7 3 9 7 4 4 7 5 5 6 5 1 4 1 9 7 8 8 2 5 4 0 7 5 4 1 5 2 7 0 7 9 0 8\n",
      " 7 5 0 2 5 8 1 5 7 7 7 5 2 2 0 4 7 8 6 7 0 2 6 8 5 7 7 8 1 7 2 0 1 9 2 1 8\n",
      " 4 8 8 6 0 1 8 5 7 8 9 3 3 0 2 3 3 3 8 4 4 9 3 1 6 7 5 5 8 0 3 0 9 3 7 2 0\n",
      " 1 7 4 5 6 5 3 5 3 9 1 4 0 1 0 1 7 6 0 9 0 7 1 8 7 0 1 3 4 4 1 1 5 1 6 7 8\n",
      " 4 8 4 2 8 7 7 5 5 7 8 2 2 8 8 0 0 4 0 6 3 0 2 7 6 8 7 1 0 8 4 5 1 8 5 3 5\n",
      " 0 7 7 0 4 9 4 3 2 6 1 7 5 3 3 4 8 3 1 4 3 4 5 5 9 6 2 6 6 7 0 4 1 3 3 5 4\n",
      " 3 2 0 8 5 2 6 1 1 9 7 8 5 1 3 9 4 4 2 2 4 9 6 2 1 3 5 7 2 8 4 6 8 2 8 2 9\n",
      " 8 8 0 8 4 9 5 4 6 6 7 0 0 3 6 6 3 6 0 9 0 9 3 4 6 2 0 0 2 9 9 0 4 7 6 2 0\n",
      " 6 0 5 9 1 3 3 1 2 7 9 0 1 2 1 9 3 0 1 5 3 3 4 8 6 1 9 9 9 8 7 8 5 4 0 0 4\n",
      " 8 3 4 7 5 8 4 7 1 0 0 7 9 2 3 6 6 6 6 8 5 5 5 1 4 4 3 5 9 9 4 8 7 7 3 9 1\n",
      " 0 4 2 3 8 3 7 4 4 7 6 8 0 4 9 6 2 8 7 2 3 7 3 7 9 2 2 1 4 9 3 5 3 4 3 6 5\n",
      " 8 4 7 7 1 8 7 4 3 6 1 0 4 6 8 0 4 5 5 8 1 7 9 6 9 5 5 8 0 4 0 9 3 7 8 6 9\n",
      " 2 2 3 1 1 1 0 9 5 1 3 2 0 3 1 4 7 8 2 2 1 0 0 7 7 1 1 2 3 6 7 8 2 5 4 9 5\n",
      " 6 3 5 0 4 7 7 6 0 4 3 7 5 8 2 1 2 9 8 6 7 6 8 4 8 0 5 5 4 4 4 1 9 8 8 9 4\n",
      " 1 0 1 8 3 2 1 1 8 1 3 6 4 0 4 9 6 7 6 6 3 6 4 6 7 5 0 0 0 4 7 6 2 1 5 4 5\n",
      " 6 1 1 9 9 4 5 0 5 0 7 2 5 5 3 9 1 7 3 7 1 9 0 0 1 9 6 0 6 9 1 9 7 0 9 1 3\n",
      " 1] [3 7 0 5 0 7 4 2 9 0 6 2 1 1 6 5 9 6 1 2 1 1 9 9 5 2 5 7 9 2 5 1 5 8 1 3 0\n",
      " 1 2 6 2 7 1 0 5 9 9 2 6 8 4 8 8 6 5 6 9 4 8 5 4 9 0 7 2 6 0 6 3 1 3 6 8 2\n",
      " 8 2 1 1 5 2 5 9 9 6 5 5 6 4 3 8 8 2 8 5 3 0 0 6 2 6 6 7 4 8 8 7 2 2 5 1 4\n",
      " 2 3 7 2 2 6 7 1 2 8 4 4 1 8 7 0 9 2 1 6 0 2 4 3 9 3 0 1 0 9 6 5 3 3 6 0 0\n",
      " 0 5 3 9 4 1 8 7 4 1 2 6 6 6 9 5 6 8 0 8 1 0 6 5 6 9 6 3 9 5 2 7 0 3 8 3 3\n",
      " 8 5 6 6 6 9 0 1 6 8 8 0 8 5 6 8 7 9 2 1 6 4 0 8 9 7 3 6 2 5 1 0 9 3 8 6 4\n",
      " 3 7 4 4 7 8 4 3 8 1 7 7 3 1 5 9 8 9 3 9 4 5 6 9 0 7 7 4 8 4 4 8 5 2 2 8 4\n",
      " 6 2 9 2 1 4 3 4 5 2 1 0 9 8 6 1 8 0 4 1 2 1 4 1 1 4 1 8 3 8 7 4 0 8 1 5 4\n",
      " 2 0 9 4 1 1 9 2 0 8 2 1 3 7 1 0 3 7 9 6 8 7 4 9 7 8 1 0 3 3 6 3 3 7 3 2 3\n",
      " 2 1 6 4 9 5 4 0 9 1 0 8 5 0 1 9 7 3 3 3 7 9 4 0 3 9 5 2 2 6 5 7 2 4 5 7 1\n",
      " 5 1 6 2 8 1 3 2 0 6 8 1 1 5 2 8 6 4 6 0 9 5 7 5 6 9 2 3 7 1 4 3 6 0 9 2 6\n",
      " 7 5 8 7 3 9 7 4 4 7 5 5 6 5 1 4 1 9 7 8 8 2 5 9 0 7 5 4 1 5 2 9 0 7 4 0 8\n",
      " 7 5 0 2 5 8 1 5 7 7 7 5 2 2 0 4 9 8 6 7 0 2 6 8 5 7 7 8 8 7 2 0 1 9 1 1 8\n",
      " 4 8 8 6 0 1 8 0 7 8 7 5 3 0 2 3 2 3 8 4 4 4 3 1 6 7 5 5 8 0 3 0 9 3 7 2 0\n",
      " 1 7 4 5 6 5 3 9 3 9 1 4 0 1 0 1 7 6 0 9 0 7 1 8 2 0 1 3 4 4 1 1 8 6 6 7 5\n",
      " 4 3 4 2 8 7 7 5 5 7 3 2 8 5 8 0 0 4 0 6 3 0 2 7 6 8 7 1 0 8 4 5 1 8 5 3 3\n",
      " 0 7 7 0 4 9 4 3 2 6 1 7 5 3 5 4 7 3 1 4 3 4 5 5 9 6 2 6 6 7 0 4 1 3 2 5 4\n",
      " 3 2 0 8 5 2 6 1 1 9 7 5 5 1 8 9 4 4 2 2 4 9 6 5 1 3 8 7 2 8 3 2 8 2 8 2 9\n",
      " 5 8 0 1 4 8 5 4 2 6 7 0 0 3 6 6 3 6 0 9 0 9 3 4 2 2 0 0 2 9 9 0 4 7 6 2 0\n",
      " 6 0 8 9 1 3 3 1 2 7 9 0 1 2 1 9 3 0 1 5 3 3 4 8 4 1 9 7 9 2 7 8 5 4 0 0 4\n",
      " 8 3 4 7 5 8 4 7 1 0 0 7 9 3 8 6 6 6 6 2 5 5 5 1 4 4 8 5 9 9 4 9 7 7 3 7 1\n",
      " 0 4 6 3 8 3 9 2 4 7 6 8 5 4 9 6 2 8 7 2 3 7 3 7 9 2 2 5 4 4 8 9 3 4 3 6 0\n",
      " 6 4 7 7 1 8 7 4 3 8 1 0 4 6 8 0 4 5 5 9 1 7 9 2 9 5 5 8 0 9 0 9 3 7 8 6 9\n",
      " 2 2 5 1 1 1 0 9 0 1 3 2 0 5 1 2 7 8 2 7 1 0 0 7 7 1 1 2 3 6 7 3 2 5 4 9 3\n",
      " 6 3 5 0 4 7 7 6 9 4 5 7 5 8 2 1 2 9 8 6 7 4 5 3 8 0 9 5 4 4 4 1 9 8 8 9 4\n",
      " 1 0 1 8 3 2 1 1 8 1 5 6 4 0 4 9 6 9 6 6 3 6 4 6 7 5 0 0 0 6 7 6 2 1 5 4 3\n",
      " 6 1 1 9 9 4 5 0 5 3 7 2 8 5 3 9 1 7 3 7 1 9 0 0 1 9 6 0 6 9 1 9 7 0 9 1 3\n",
      " 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.852"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seeing how the model works on the test data\n",
    "pred_test = make_predictions(X_test, W1, b1, W2, b2)\n",
    "get_accuracy(pred_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2977a-eacc-4217-9a31-f509b13b3ece",
   "metadata": {},
   "source": [
    "#### Accuracy on Test data: ~85%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
